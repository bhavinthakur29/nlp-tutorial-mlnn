{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual assignment: Machine learning tutorial\n",
    "# Name - Bhavin Thakur (23079699)\n",
    "# GitHub - https://github.com/bhavinthakur29/nlp-tutorial-mlnn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Drop missing values if any\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Map sentiment labels to numeric values (e.g., Positive = 1, Negative = 0)\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "max_length = 100\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74765118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_length))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)))) # Bidirectional for better context learning\n",
    "model.add(Dropout(0.6))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid')) # Sigmoid for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.0005),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2691a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_padded, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(test_padded, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0773d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of training vs validation accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='#377eb8', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='#ff7f00', marker='s')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot of training vs validation loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='#4daf4a', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='#e41a1c', marker='s')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16145afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment on test data\n",
    "y_pred = (model.predict(test_padded) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da691f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Prediction\n",
    "new_review = [\"The movie was absolutely wonderful!\"]\n",
    "new_seq = tokenizer.texts_to_sequences(new_review)\n",
    "new_pad = pad_sequences(new_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "prediction = model.predict(new_pad)[0][0]\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print(\"Positive Review\")\n",
    "else:\n",
    "    print(\"Negative Review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe17160",
   "metadata": {},
   "source": [
    "----\n",
    "## References\n",
    "- Goldberg, Y. (2017). **Neural Network Methods for Natural Language Processing**. *Synthesis Lectures on Human Language Technologies*. [Link](https://doi.org/10.2200/S00762ED1V01Y201703HLT037)\n",
    "- Hochreiter, S., & Schmidhuber, J. (1997). **Long Short-Term Memory**. *Neural Computation, 9(8), 1735-1780*. [Link](https://doi.org/10.1162/neco.1997.9.8.1735)\n",
    "- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**. *arXiv preprint*. [Link](https://arxiv.org/abs/1810.04805)\n",
    "- Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). **Distributed Representations of Words and Phrases and their Compositionality**. *Advances in Neural Information Processing Systems (NeurIPS)*. [Link](https://arxiv.org/abs/1310.4546)\n",
    "- Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., & Potts, C. (2013). **Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank**. *Conference on Empirical Methods in Natural Language Processing (EMNLP)*. [Link](https://www.aclweb.org/anthology/D13-1170/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
